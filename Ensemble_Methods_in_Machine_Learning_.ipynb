{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FVafhtEbWE2U",
        "RTvBBX9rI9Fz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HagarIbrahiem/Ensemble-Methods-in-Machine-Learning/blob/main/Ensemble_Methods_in_Machine_Learning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Intro"
      ],
      "metadata": {
        "id": "BMKxCRosUsvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The datasets used in this tutorial are Sklearn:\n",
        "- `Make_moon` Dataset for Classification\n",
        "- `Diabetes` Dataset for Regression\n"
      ],
      "metadata": {
        "id": "7hkPHR-UUvHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libs"
      ],
      "metadata": {
        "id": "pJz3DfTeUvaq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MIgJ7c8JUdbk"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "## datasets\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "\n",
        "## ML models\n",
        "# classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "# Ensemble\n",
        "from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor , GradientBoostingRegressor\n",
        "from sklearn.ensemble import VotingClassifier,VotingRegressor,BaggingClassifier, StackingRegressor\n",
        "\n",
        "## metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score ,  r2_score ,  mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "## ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "FVafhtEbWE2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the make_moons dataset - classification\n",
        "X, y =make_moons(n_samples=10000, noise=0.4)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X, y, random_state = 10, test_size = 0.30)\n"
      ],
      "metadata": {
        "id": "Nc_jJ9DuWDy0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the diabetes dataset - regression\n",
        "diabetes = datasets.load_diabetes(as_frame=True)\n",
        "\n",
        "# Separate out the data\n",
        "X_diabetes = diabetes['data']\n",
        "y_diabetes = diabetes['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = train_test_split(X_diabetes, y_diabetes, random_state = 50, test_size = 0.2)"
      ],
      "metadata": {
        "id": "_GcSMOSuZk8S"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification - Single Predictor - Iris Dataset"
      ],
      "metadata": {
        "id": "ur63KrxUXeGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_decisionTreeClassifier = DecisionTreeClassifier()\n",
        "\n",
        "t0 = time.time()\n",
        "_decisionTreeClassifier.fit(X_train_iris, y_train_iris)\n",
        "Acc_dt_iris = accuracy_score(y_test_iris, _decisionTreeClassifier.predict(X_test_iris))\n",
        "t1 = time.time()\n",
        "\n",
        "print(\"Acc-Decision Tree:\",Acc_dt_iris )\n",
        "print(\"Computation Time: {}\".format(round(t1-t0 , 4))  , '\\n')\n"
      ],
      "metadata": {
        "id": "Rw9w3zzRWG2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d357a90a-2e22-4725-d363-ab2817c4a888"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc-Decision Tree: 0.8006666666666666\n",
            "Computation Time: 0.0256 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_decisionTreeClassifier = DecisionTreeClassifier()\n",
        "\n",
        "t0 = time.time()\n",
        "_decisionTreeClassifier.fit(X_train_iris, y_train_iris)\n",
        "Acc_dt_iris = accuracy_score(y_test_iris, _decisionTreeClassifier.predict(X_test_iris))\n",
        "t1 = time.time()\n",
        "\n",
        "print(\"Acc-Decision Tree:\",Acc_dt_iris )\n",
        "print(\"Computation Time: {}\".format(round(t1-t0 , 4))  , '\\n')\n"
      ],
      "metadata": {
        "id": "Gz1cRIQe5mf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10a9a69-7149-489a-9827-3a33aa6a2b1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc-Decision Tree: 0.802\n",
            "Computation Time: 0.0315 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zw7C24lj5m2b"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression - Single Predictor - Diabetes Dataset"
      ],
      "metadata": {
        "id": "p-JOGY9PRagj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_reg = DecisionTreeRegressor()\n",
        "\n",
        "t0 = time.time()\n",
        "dt_reg.fit(X_train_diabetes, y_train_diabetes)\n",
        "y_pred = dt_reg.predict(X_test_diabetes)\n",
        "t1 = time.time()\n",
        "\n",
        "print(\"RMSE-DecisionTreeRegressor:\",mean_squared_error(y_test_diabetes, y_pred , squared=False) )\n",
        "print(\"Computation Time: {}\".format(round(t1-t0 , 4))  , '\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLKaBzQxRb7w",
        "outputId": "8ab9ade7-4aa8-44af-8d4d-bafb697c3dc0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE-DecisionTreeRegressor: 80.05510742431804\n",
            "Computation Time: 0.0103 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble Methods**"
      ],
      "metadata": {
        "id": "kP8D829WReau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Voting\n",
        "\n",
        "Train different ML Predictors on same dataset:\n",
        "- Voting Classifier - Iris dataset\n",
        "- Voting Regressor - Diabates dataset"
      ],
      "metadata": {
        "id": "5nssyqKX1iNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting Classifier"
      ],
      "metadata": {
        "id": "P1lpvL6CRHBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "xgboost_clf = XGBClassifier()\n",
        "voting_clf = VotingClassifier( estimators=[('lr', log_clf), ('rf', rnd_clf), ('xgb', xgboost_clf)], voting='hard')\n",
        "\n",
        "for clf in (log_clf, rnd_clf, xgboost_clf, voting_clf ):\n",
        "   t0 = time.time()\n",
        "   clf.fit(X_train_iris, y_train_iris)\n",
        "   y_pred = clf.predict(X_test_iris)\n",
        "   t1 = time.time()\n",
        "\n",
        "   print('Acc-',clf.__class__.__name__, ':', accuracy_score(y_test_iris, y_pred) )\n",
        "   print(\"Computation Time: {}\".format(round(t1-t0,4))  , '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfsOnvb-1DHC",
        "outputId": "b17aa88c-4b90-49ac-f4a9-483b76c2ef3c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc- LogisticRegression : 0.8353333333333334\n",
            "Computation Time: 0.0113 \n",
            "\n",
            "Acc- RandomForestClassifier : 0.853\n",
            "Computation Time: 1.0809 \n",
            "\n",
            "Acc- XGBClassifier : 0.8606666666666667\n",
            "Computation Time: 0.4673 \n",
            "\n",
            "Acc- VotingClassifier : 0.8623333333333333\n",
            "Computation Time: 1.5495 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There you have it! The voting classifier slightly outperforms all the individual classifiers.**"
      ],
      "metadata": {
        "id": "GGk1R7k-6Rag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hard Vs. Soft Voting"
      ],
      "metadata": {
        "id": "AxAqsC7vQiHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC( probability = True) # to enable predict_propba()\n",
        "xgboost_clf = XGBClassifier()\n",
        "voting_clf = VotingClassifier( estimators=[('lr', log_clf), ('rf', rnd_clf), ('xgb', xgboost_clf)], voting='soft') ## soft voting\n",
        "\n",
        "\n",
        "for clf in (log_clf, rnd_clf,xgboost_clf, voting_clf ):\n",
        "   t0 = time.time()\n",
        "   clf.fit(X_train_iris, y_train_iris)\n",
        "   y_pred = clf.predict(X_test_iris)\n",
        "   t1 = time.time()\n",
        "   print('Acc-',clf.__class__.__name__,':', accuracy_score(y_test_iris, y_pred))\n",
        "   print(\"Computation Time: {}\".format(round(t1-t0,4))  , '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cdvk9rlRojH",
        "outputId": "510a91cb-4e8f-4351-a2d7-e1514417f208"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc- LogisticRegression : 0.8353333333333334\n",
            "Computation Time: 0.0113 \n",
            "\n",
            "Acc- RandomForestClassifier : 0.855\n",
            "Computation Time: 1.0847 \n",
            "\n",
            "Acc- XGBClassifier : 0.8606666666666667\n",
            "Computation Time: 0.4497 \n",
            "\n",
            "Acc- VotingClassifier : 0.8633333333333333\n",
            "Computation Time: 3.2065 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting Regressor"
      ],
      "metadata": {
        "id": "I30QBWogRNHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin_reg = LinearRegression()\n",
        "rnd_reg = RandomForestRegressor()\n",
        "svr_reg = SVR( )\n",
        "dr_reg = DecisionTreeRegressor()\n",
        "knn_reg = KNeighborsRegressor()\n",
        "voting_reg = VotingRegressor( estimators=[ ('lr', lin_reg), ('rf', rnd_reg), ('svr', svr_reg) , ('dt', dr_reg) , ('knn',knn_reg)])\n",
        "\n",
        "\n",
        "for reg in (lin_reg, rnd_reg, svr_reg,dr_reg,knn_reg, voting_reg ):\n",
        "   t0 = time.time()\n",
        "   reg.fit(X_train_diabetes, y_train_diabetes)\n",
        "   y_pred = reg.predict(X_test_diabetes)\n",
        "   t1 = time.time()\n",
        "\n",
        "   print('RMSE-',reg.__class__.__name__,':', mean_squared_error(y_test_diabetes, y_pred , squared=False) )\n",
        "   print(\"Computation Time: {}\".format(round(t1-t0,4))  , '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4icFlbDFRjf2",
        "outputId": "bc915098-ac4f-48b0-9068-27107d9c9507"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE- LinearRegression : 51.487471054434295\n",
            "Computation Time: 0.007 \n",
            "\n",
            "RMSE- RandomForestRegressor : 56.740281858599\n",
            "Computation Time: 0.3296 \n",
            "\n",
            "RMSE- SVR : 67.43165774761839\n",
            "Computation Time: 0.0139 \n",
            "\n",
            "RMSE- DecisionTreeRegressor : 78.10573357940842\n",
            "Computation Time: 0.0057 \n",
            "\n",
            "RMSE- KNeighborsRegressor : 58.31968465258988\n",
            "Computation Time: 0.0066 \n",
            "\n",
            "RMSE- VotingRegressor : 55.74637330472197\n",
            "Computation Time: 0.3383 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Bagging\n",
        "\n",
        "Train same ML Predictors on different subsets (With Replacments)"
      ],
      "metadata": {
        "id": "pF485NcP-32n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define base predictor and bagging classifier\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500 , bootstrap=True) #base predictor may be = XGBClassifier() or any other.\n",
        "\n",
        "t0 = time.time()\n",
        "bag_clf.fit(X_train_iris, y_train_iris)\n",
        "y_pred = bag_clf.predict(X_test_iris)\n",
        "t1 = time.time()\n",
        "\n",
        "print(\"Acc-Bagging Classifier :\",accuracy_score(y_test_iris, y_pred ) )\n",
        "print(\"Computation time:\", round(t1-t0,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cmx43FM8Cn_",
        "outputId": "0d535b2f-a16a-4041-bfee-8aff5e03f0b3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc-Bagging Classifier : 0.852\n",
            "Computation time: 8.3594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Pasting\n",
        "\n",
        "Train same ML Predictors on different subsets (Without Replacments)"
      ],
      "metadata": {
        "id": "r3In9ObP-81N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define base predictor and bagging classifier\n",
        "pasting_clf = BaggingClassifier( XGBClassifier (), n_estimators = 300 , bootstrap=False) # if you want to use pasting instead, just set bootstrap=False\n",
        "\n",
        "t0 = time.time()\n",
        "pasting_clf.fit(X_train_iris, y_train_iris)\n",
        "y_pred_pasting= pasting_clf.predict(X_test_iris)\n",
        "t1 = time.time()\n",
        "time_pasting_iris = round(t1-t0 , 4)\n",
        "\n",
        "Acc_pasting_iris = accuracy_score(y_test_iris, y_pred_pasting )\n",
        "\n",
        "print(\"Acc-Pasting Classifier :\",Acc_pasting_iris )\n",
        "print(\"Computation time:\", time_pasting_iris)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEPNrkjK-7S6",
        "outputId": "90514782-2a8e-4daf-ffe1-66b38c5be641"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc-Pasting Classifier : 0.8606666666666667\n",
            "Computation time: 159.0112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same code of Bagging/Pasting Classifiers using DecisionTreeClassifier as a base estimator can be replaced with the following code using Randome Forest.\n",
        "\n",
        " Random forest algorithm itself  is an ensemble learning technique combining the outputs of numerous decision trees classifiers to enhance a modelâ€™s performance, generally trained via the bagging method (or sometimes pasting) , thus instead of building a BaggingClassifier and passing\n",
        "it a DecisionTreeClassifier, you can instead use the RandomForestClassifier\n",
        "class, which is more convenient and optimized for Decision Trees\n"
      ],
      "metadata": {
        "id": "3zg0Eq-3BKPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "_randomForestClassifier = RandomForestClassifier(n_estimators=500)\n",
        "\n",
        "t0 = time.time()\n",
        "_randomForestClassifier.fit(X_train_iris, y_train_iris)\n",
        "y_pred_rf= _randomForestClassifier.predict(X_test_iris)\n",
        "t1 = time.time()\n",
        "\n",
        "print(\"Acc-Random Forest :\",accuracy_score(y_test_iris, y_pred_rf ) )\n",
        "print(\"Computation time:\", round(t1-t0 , 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9A7J8Ut6hqV",
        "outputId": "53c5361e-eb9b-410a-9150-7f4ef9ea4b91"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc-Random Forest : 0.8536666666666667\n",
            "Computation time: 5.1017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Stacking"
      ],
      "metadata": {
        "id": "pQzbxFsfF7lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Implement Stacking using (np.column_stack)**"
      ],
      "metadata": {
        "id": "RTvBBX9rI9Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base models\n",
        "models = [\n",
        "    ('lr', LinearRegression()),\n",
        "    ('dt', DecisionTreeRegressor()),\n",
        "    ('gb', GradientBoostingRegressor(random_state=42))\n",
        "]\n",
        "\n",
        "# Train the base models and make predictions\n",
        "base_predictions = []\n",
        "for name, model in models:\n",
        "    model.fit(X_train_diabetes, y_train_diabetes)\n",
        "    predictions = model.predict(X_test_diabetes)\n",
        "    base_predictions.append(predictions)\n",
        "\n",
        "# Create a meta-model (second-level model) to combine the base model predictions\n",
        "meta_model = LinearRegression()\n",
        "meta_X = np.column_stack(base_predictions)\n",
        "\n",
        "# Train the meta-model on the base model predictions\n",
        "meta_model.fit(meta_X, y_test_diabetes)\n",
        "\n",
        "# Make predictions using the stacked ensemble\n",
        "stacked_predictions = []\n",
        "for predictions in base_predictions:\n",
        "    stacked_predictions.append(predictions)\n",
        "stacked_predictions = np.column_stack(stacked_predictions)\n",
        "ensemble_predictions = meta_model.predict(stacked_predictions)\n",
        "\n",
        "# Calculate the RMSE for the ensemble prediction\n",
        "ensemble_rmse = mean_squared_error(y_test_diabetes, ensemble_predictions,  squared=False)\n",
        "print(\"Ensemble Staking RMSE:\", ensemble_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5PG-Q0gHAgM",
        "outputId": "f975b631-3504-4ac0-9588-3f79bb45cfc6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Staking RMSE: 50.449000640632555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing stacking, we do obtain a better result compared to individual models!**"
      ],
      "metadata": {
        "id": "9EqbiQAaAfHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble of ensembles ( using 2 MetaModels)"
      ],
      "metadata": {
        "id": "qtY580EgKMqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base models\n",
        "base_models = [\n",
        "    ('lr', LinearRegression()),\n",
        "    ('dt', DecisionTreeRegressor()),\n",
        "    ('gb', GradientBoostingRegressor())\n",
        "]\n",
        "\n",
        "\n",
        "# Define the first-level meta-models\n",
        "meta_models = [\n",
        "    LinearRegression(),\n",
        "    RandomForestRegressor()\n",
        "]\n",
        "\n",
        "# Train the base models and make predictions\n",
        "base_predictions = []\n",
        "for name, model in base_models:\n",
        "    model.fit(X_train_diabetes, y_train_diabetes)\n",
        "    predictions = model.predict(X_test_diabetes)\n",
        "    base_predictions.append(predictions)\n",
        "\n",
        "# Create the inputs for the second-level meta-models\n",
        "meta_inputs = np.column_stack(base_predictions)\n",
        "\n",
        "# Train the second-level meta-models\n",
        "ensemble_predictions = []\n",
        "for model in meta_models:\n",
        "    model.fit(meta_inputs, y_test_diabetes)\n",
        "    predictions = model.predict(meta_inputs)\n",
        "    ensemble_predictions.append(predictions)\n",
        "\n",
        "# Take the average of predictions from the second-level meta-models\n",
        "ensemble_predictions = np.mean(ensemble_predictions, axis=0)\n",
        "\n",
        "# Calculate the RMSE for the ensemble prediction\n",
        "ensemble_rmse = np.sqrt(mean_squared_error(y_test_diabetes, ensemble_predictions))\n",
        "print(\"Ensemble RMSE:\", ensemble_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K_LGvHNKNa4",
        "outputId": "f87717ce-5367-44dd-f419-0f0e548b9bab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble RMSE: 34.41134490607928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble of ensembles ( using 3 MetaModels)"
      ],
      "metadata": {
        "id": "DLsXSCV-Ln51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base models\n",
        "base_models = [\n",
        "    ('lr', LinearRegression()),\n",
        "    ('dt', DecisionTreeRegressor()),\n",
        "    ('gb', GradientBoostingRegressor())\n",
        "]\n",
        "\n",
        "\n",
        "# Define the first-level meta-models\n",
        "meta_models = [\n",
        "    LinearRegression(),\n",
        "    RandomForestRegressor(),\n",
        "    GradientBoostingRegressor()\n",
        "]\n",
        "\n",
        "# Train the base models and make predictions\n",
        "base_predictions = []\n",
        "for name, model in base_models:\n",
        "    model.fit(X_train_diabetes, y_train_diabetes)\n",
        "    predictions = model.predict(X_test_diabetes)\n",
        "    base_predictions.append(predictions)\n",
        "\n",
        "# Create the inputs for the second-level meta-models\n",
        "meta_inputs = np.column_stack(base_predictions)\n",
        "\n",
        "# Train the second-level meta-models\n",
        "ensemble_predictions = []\n",
        "for model in meta_models:\n",
        "    model.fit(meta_inputs, y_test_diabetes)\n",
        "    predictions = model.predict(meta_inputs)\n",
        "    ensemble_predictions.append(predictions)\n",
        "\n",
        "# Take the average of predictions from the second-level meta-models\n",
        "ensemble_predictions = np.mean(ensemble_predictions, axis=0)\n",
        "\n",
        "# Calculate the RMSE for the ensemble prediction\n",
        "ensemble_rmse = np.sqrt(mean_squared_error(y_test_diabetes, ensemble_predictions))\n",
        "print(\"Ensemble RMSE:\", ensemble_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkJ6sJl1KxDH",
        "outputId": "83bf0378-5610-40ce-bad4-a19da440b247"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble RMSE: 27.11790360620823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Boosting"
      ],
      "metadata": {
        "id": "oS1KUQ7dLxWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are :\n",
        "-\tAdaBoost\n",
        "-\tGradient Boost\n",
        "-\tLight GBM\n",
        "-\tXGBoost\n",
        "-\tCatBoost\n"
      ],
      "metadata": {
        "id": "ErSNov4wL5sf"
      }
    }
  ]
}